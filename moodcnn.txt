Testing
- Mood detection
	- let user select mood manually
	- mood by radio button only
	- block next if no input
	

- database
	- admin made(superuser login give admin panel)?
	- user register valid email?
	- user register invalid email?
	- user register?
	- admin and user login?

-song
	-song recommend even with manual mood selection
	- let user select songs from recommendations
	- when mood change, song select clear

-login spotify
	- login button work?
	- authorize valids?
	- unuhorize unvalids?
	- callback page gone work?



Things to do
songs
mood statistics
database
dashboard: what songs what post check, 

classes files:
model.py
views.py
moodcnn_pytorch.ipynb
serializers.py




For Mood statistics
User-Level Analytics

Mood Trends Over Time
Line chart showing how the user’s moods change over days/weeks/months.

Most Frequent Mood
Pie chart showing distribution of moods the user records.

Songs Played per Mood
Bar chart showing how many times each song was played for a particular mood.

Mood Detection Accuracy (if comparing manual vs auto detection)
Scatter plot or bar chart showing correct vs incorrect mood detection.

Average Songs Selected per Mood
Bar chart showing how many songs the user typically selects per mood.


2. Global / Aggregate Analytics

Overall Mood Distribution

Pie or donut chart showing what percentage of moods are happy, sad, angry, etc.

Popular Songs by Mood

Top 10 songs per mood in a bar chart or leaderboard style.

Mood Trends Over Time Across Users

Line chart showing changes in mood frequency across all users by day/week/month.

Active Users by Mood Activity

Bar chart or heatmap showing which users log moods most frequently.

Songs Trending for a Mood

List or bar chart showing which songs are trending for a particular mood recently.

______________________________________________________

1. Fetch songs for a post’s mood detection

Goal: After a user uploads an image (or a drawing), the CNN detects a mood → you want songs that match that mood.

Spotify APIs to use:

Search API: GET /v1/search?q={mood}&type=track

Query Spotify for songs that match the mood keyword (e.g., "happy", "sad", "energetic").

Use access token from Spotify OAuth for authorized requests.

Tracks API: GET /v1/tracks/{id}

Once you have track IDs, get detailed info: name, album, artists, duration, genre, cover image.

Where in your project:

Backend endpoint /api/get_tracks_for_mood/

Input: mood label (from CNN)

Output: list of tracks → store them in Track model linked to the Post.

Notes:

You can fetch multiple tracks and allow the user to select which ones are linked to the post.

Tracks are stored in your DB so that multiple posts can share the same track.
__________________________________

2. Homepage music player (playback & top listens)

Goal: A user logs in → see a music player that plays either:

The top songs you’ve served to them

Their “top listens” from your app

Spotify APIs to use:

Web Playback SDK (optional if you want playback in browser):

Play Spotify songs directly in your frontend music player.

Requires the user to be authenticated with Spotify.

Player API (PUT /v1/me/player/play)

Control playback: play, pause, skip tracks.

Top Tracks API (GET /v1/me/top/tracks)

If you want personalized Spotify top tracks for the user (requires Spotify account login).

Where in your project:

Frontend: React Home page music player component.

Backend: optional endpoint to proxy requests to Spotify API with session token.

Notes:

Users must connect their Spotify account to use real playback features.

If they don’t connect, you can still play tracks from your database (Track model) using embedded audio URLs or previews.
___________________________________

3. Favorite music page

Goal: Let users see songs they’ve favorited in your app.

Spotify APIs to use:

Only if you want to check actual Spotify info (like album art or preview URL)

Otherwise, use local database tracks:

TrackFavorite model: user → tracks relationship

Render track details (name, album, image_url, artists)

Where in your project:

Frontend: React FavoriteMusic page

Backend: /api/favorites/ → return user’s favorited tracks

Notes:

No need to call Spotify API for every favorite track if you already stored track info in DB.


the saving thing
# Save MoodDetection (link to request.user if available)
    try:
        user = request.user if getattr(request, "user", None) and request.user.is_authenticated else None
        md = MoodDetection.objects.create(
            user=user,
            mood=mood,
            confidence=confidence,
            image_path=saved_name or (image_url or ""),
        )
    except Exception:
        logger.exception("Failed saving MoodDetection")





In your `MoodCNN`, they used **4 convolutional layers** and **2 fully connected (FC) layers** for a few key reasons:

1. **Progressive Feature Extraction (4 Conv Layers):**

   * Each convolutional layer extracts increasingly complex features.
   * The first layer (16 filters) captures very simple patterns like edges or corners.
   * Middle layers (32, 64 filters) capture shapes or textures.
   * The last layer (128 filters) captures high-level, abstract features that are more meaningful for classification (like facial expressions).
   * More layers → richer hierarchical feature representation.

2. **Flattening → Fully Connected Layers (2 FC Layers):**

   * After convolutions, the feature maps are flattened into a vector.
   * The first FC layer (128 neurons) acts as a **hidden layer**, learning combinations of the features extracted by the conv layers.
   * The second FC layer maps these learned combinations to the **final output classes** (num_classes).
   * Using two FC layers instead of one gives the network slightly more modeling capacity to learn complex mappings without being too heavy.

3. **Regularization and Nonlinearity:**

   * ReLU between layers allows modeling non-linear relationships.
   * Dropout and pooling in conv layers reduce overfitting, allowing more layers without drastically hurting generalization.

✅ **In short:** 4 conv layers help capture hierarchical patterns in images, and 2 FC layers let the network combine those features effectively to predict the final classes.

If you want, I can also make a **diagram showing how features flow through conv → FC layers** so it’s super easy to visualize. Do you want me to do that?






Post table:

user_id → User relation

image_path → from imageUrl

canvas_image / canvas_data → optional if you have drawing feature later

Tracks for post (many-to-many Post.track):

Use selectedSongs.map(s => spotify_id)

Only attach existing Track objects

Mood detection (optional):

MoodDetection.user → userId

MoodDetection.post → postId

MoodDetection.mood → mood or manualMood

MoodDetection.confidence → if your API provides a confidence score

